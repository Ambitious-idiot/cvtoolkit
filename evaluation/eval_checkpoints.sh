#!/bin/bash
# Run this script to evaluate all the checkpoints generated by an experiment
# Assuming the directory is like
# -configs
# --experiment1.py
# --experiment2.py
# -output
# --experiment1
# ---model_1.pth
# ---model_2.pth
# --experiment2
# ---model_1.pth
# ---model_2.pth
# -test_logs
# Usage:
# ```bash
# bash eval_checkpoints.sh $experiment_name
# ```
# Variables
experiment_name="$1" # Input the experiment name, which is both the name of config file and output diretory, as the first command argument
evaluater="/path/to/eval.py" # Python script to run the evaluation
log_dir="/path/to/log_directory"
batch_size=128

# Set config and log
config_file="configs/$experiment_name.py"
checkpoint_files=($(ls "output/$experiment_name" | grep "model_[0-9]\+\.pth"))
mkdir -p "$log_dir"
log_file="test_logs/$experiment_name.log"
echo "Evaluate with config: $config_file, and logged in $log_file"

> $log_file

run_experiment() {
    local checkpoint="$1"
    echo "Evaluating with checkpoint: $checkpoint" >> "$log_file"
    python $evaluater \
    --config_file $config_file \
    --batch_size $batch_size \
    --model_weights "$checkpoint"\
     >> "$log_file" 2>&1
}

for checkpoint in "${checkpoint_files[@]}"
do
    run_experiment "output/$experiment_name/$checkpoint" &
    pid=$!
    wait "$pid"
    sleep 5
done
echo done!
